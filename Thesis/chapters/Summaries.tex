\chapter{Overview of the Papers}
In this chapter, we give an overview of the four papers included in the thesis. Marie Dubremetz has been mainly responsible for all four papers, and is solely responsible for all work on implementation and evaluation. Joakim Nivre has contributed to design, analysis, annotation and preparation of the final text.
\section*{\Pref{paper1}}
In this paper we develop our first hand-tuned system for chiasmus detection. We introduce the notion of chiasmus as a graded phenomenon as well as our method of annotation with True, False and Borderline labels. We test shallow features based on stopwords, punctuation, n-grams, and lexical clues. We show that our model does fairly well on this task and improve on pre-existing methods, both in terms of recall and precision.

\section*{\Pref{paper2}} 
This paper builds on \Pref{paper1} in order to improve detection. The system we develop reuses the same schema of annotation and the same type of model: linear regression. We test two categories of deep features: the part-of-speech tag similarity of words and syntactic role labels. These features appear to increase the performance of the detection on our political discourse corpus by nearly 25\% in absolute terms compared to the previous system. By extending the annotation to two annotators  we show that the inter-annotator agreement on chiasmus is good. That  proves that chiasmus is not an idiosyncratically defined notion, at least on the prototypical level. %We finally test our system on the genre of novel and observe that the performance of our model generalises on it. We show that the deep features may increase average precision from about 40 to 65\%. And the good inter-annotator agreement score proves that chiasmus is not an notion just individually defined at least on prototypical cases. 


\section*{\Pref{paper3}}
This paper uses data collected in our first two studies with hand-tuned models to explore a machine learning approach to tuning feature weights. We then compare the weights given by the machine and the weights given by the human. We discover two things. A partially annotated corpus (less than 1\% manually annotated) can be enough for training a system as effective as the hand-tuned one, and computers and humans generally agree on which features are positive or negative.

\section*{\Pref{paper4}}
This journal article starts from the machine learning approach developed in Paper III and extends it in two ways:
detecting new figures and exploring new text genres. We apply our machine learning approach not only to chiasmus but also to epanaphora and epiphora. We comparatively explore the three figures together on the same corpus and discover that, at extraction time, they display huge differences in number of candidates. Nevertheless, all of them suffer from a common problem: the overwhelming number of false instances generated by extraction. Therefore, we selectively annotate candidate epanaphora and epiphora using rule-based filters. After training the system on those annotated instances, we can outperform a model using only the basic features by 20 percentage points on almost all evaluation metrics. We discover that, despite their theoretical proximity, epanaphora and epiphora do not react in the same way to the same features. Thus, they are not just mirror images of each other. Finally, we illustrate how our detectors can be used to explore texts from three different genres. Thanks to the
automatic detection of rhetorical figures, we discover that chiasmus is more likely to appear in
scientific contexts, whereas epanaphora and epiphora are more common in fiction.
