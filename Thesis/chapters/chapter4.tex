\chapter{\label{chap5}Summary of Empirical Results}%
This chapter summarises the results obtained in Papers~\ref{paper1}, \ref{paper2}, \ref{paper3} and \ref{paper4}. Our objective is not to review all the results that the reader can find in our articles, but to highlight the main trends and findings.
\section{Exploration}
One of the first simple but important results of this research is the exploration of the problems generated by the figures. From Paper~I to \Pref{paper4} we illustrate by examples or annotation of random samples (\Pref{paper4}) how rare the figures are, and thus what problems we can anticipate: imbalanced corpus, difficulties of annotation. From the first paper we state how chiasmus is not just slightly infrequent, but is an extreme needle-in-the-haystack problem and thus a challenge for machine learning.  We point to the real issue, which is not to extract candidates, but how in practice we can design a decent system for detection.

Another interesting finding concerns epanaphora. In \Pref{paper4}, we observe that out of 100 randomly selected epanaphora candidates, only one is a genuine example, the rest being obviously false examples and a couple of borderline cases. A few years ago, \citet{Strommer2011} conducted a similar exploration. In order to measure statistical significance, he asked two annotators to annotate 156 randomly selected epanaphora. The interesting finding is that of those 156 examples, only 2 (i.e., $\sim$1\%) were considered as True by both annotators, and the rest were a majority of false instances annotated as such by both annotators.\footnote{Although their number of what they called ``debated'' cases (14\%) is definitely larger than our number of borderlines (3\%)} Of course, the fact that we find the exact same ratio could be a coincidence. Nevertheless, it is an interesting finding because it means that both we and this researcher agree on the rarity of the phenomenon, at least for the most prototypical cases, with a surprisingly close ratio. These exploratory annotations have been done by two independent researchers with different annotators, and different theoretical frameworks. (Strommer, unlike us, prefers to speak of ``intentional'' repetition.) Yet the empirical observation remains the same.

The last observation that we can make about our exploration concerns both epanaphora and epiphora. We observed in \Pref{paper4} that they are not just mirror images of each other. Yes, theoretically they might be similar, but computationally and statistically they are not the same problem. This is due to grammar. The minimum definition of epanaphora (i.e., one word identical at the beginning of a sentence) allows the extraction of at least three times as many candidates as that for epiphora. English grammar favours the use of function words at the beginning of sentences (like pronouns or articles). This is nothing exceptional and it should not be seen as the excessive frequency of a figure of speech, but rather as standard use of the grammar.

To conclude this section, we can say that these observations, as simple as they may seem, are among the most important contributions of this thesis: we do not just propose solutions to detect figures of speech; we also help identify the most important research problems.
\section{Inter-Annotator Agreement}

As part of our investigations, we have estimated inter-annotator agreement for chiasmus, epanaphora and epiphora. We used the unweighted Cohen's Kappa coefficient to quantify agreement. \cite{Strommer2011} has already claimed that the agreement for epanaphora is good. We confirm it ($\kappa =0.85$ for epana\-phora).  The inter-annotator agreement is also good for epiphora ($\kappa =0.88$) and chiasmus ($\kappa =0.69$). This measurement is obtained by measuring the agreement between what our annotators think are prototypical cases (annotated as True) and are not prototypical cases (annotated as False or Borderline). 

Note that the inter-annotator agreement, while still good, is definitely lower for chiasmus than for the other figures. We see two possible reasons for that. The first is practical: chiasmus was the first figure studied. We were therefore less experienced in the annotation process than for the two other figures. A possible consequence of this may be a lack of consistency in annotations. Another factor is the difficulty of the task for chiasmus. For epanaphora and epiphora the annotation question is simple: is the repetition at the beginning/end of the sentence an instance of epanaphora/epiphora? For chiasmus the question was dual, because we had to distinguish between true instances and duplicates of true instances.\footnote{`\mn{Foul} is \mn{fair} and \mn{fair} is \mn{foul}' is a true instance whereas `Foul \mn{is} \mn{fair} and \mn{fair} \mn{is} foul' is a duplicate} The human and the computer have to decide not only if the extract is a good example but also if the words chosen for the repetitions are the main words of the chiasmus. As a result, annotators sometimes agree that the extract is correct but do not agree about which word the chiasmus is playing on. A concrete example of this kind of conflict is presented by \Eref{exRegime} extracted from our training corpus.
%
\nnumsentence{In recent years in Africa, we have not seen a shift from \mn{totalitarian} \mn{regimes} to \mn{democracy}, but quite the contrary -- a shift from \mn{democracy} to \mn{totalitarian} \mn{regimes}. \label{exRegime}}%
%
Most canonical examples of chiasmus display a strong hierarchy in their terms. Often the grammatical heads of the phrases are by default the main words and are all of the same nature. However, in this particular example, there is a conflict between semantic and grammatical hierarchy. On the one hand, ``regimes'' is the noun and thus is grammatically equivalent to ``democracy''. On the other hand, this chiasmus plays on the contradictions between the notions of democracy and totalitarianism, and one is tempted to prefer ``totalitarian'' as the main word of the chiasmus. Examples like this can influence the inter-annotator agreement.
\section{Contributions of Different Types of Features}
In this section we discuss the contribution of our thesis in terms of discovering relevant features for chiasmus, epanaphora and epiphora detection. Since our study on chiasmus is more extended, we discuss two subtypes of features: shallow and deep ones. Then we report the performance obtained with shallow features on epanaphora and epiphora.
\subsection{Chiasmus}
\paragraph{Shallow Features}
The first empirical finding is how dramatic the improvement of the chiasmus detection can be when applying some basic features. The simple extraction of every candidate in our corpus leads to a list of one million candidates. Intuitively, we start by applying the most simple filtering features we could think of: identifying stopwords and locating punctuation marks. Thanks to this, we understand that filtering is good but not discriminative enough. Too many candidates neither involve stopwords nor are divided by major punctuation marks, and thus all of these are given the same maximum possible score by the model. As a consequence, the general precision of such a simple system is far below 2\% (16 out of 1180). And, it would still take at least two hours for a trained user to read all this material before finding the real chiasmi hidden in it. This confirms how non-trivial the problem of chiasmus detection is, especially when the corpus is large (in our case, over two million words).

Three other categories of features, size-related ones (e.g., the longer the chiasmus, the lower it scores), measurement of similarity of context, and detection of recurrent lexical patterns (for instance, the presence of negation underlying a contrast) are enough to make a system useful for the user. With these features, the precision at ten candidates is 70\%, which means that to find seven chiasmi the user only had to read the top ten answers given by the machine, and most of the candidates (16 out of 19) could be found among the top 200 candidates.


\paragraph{Deep Features}
A category of features that are particularly expected in chiasmus detection are  syntax features. Indeed, if we think about the top-three most well-known chiasmi, presented in Examples~\ref{ex:Beginning}, \ref{exCountry}, \ref{exAll}, the first salient common point we notice is their perfectly symmetrical switch of syntactic roles. 

\nnumsentence{It is not the \mn{beginning} of the \mn{end}; it is the \mn{end} of the \mn{beginning}. \label{ex:Beginning}}%
\vspace{-0.6cm}
\nnumsentence{Ask not what your \mn{country} can do for \mn{you}; ask what \mn{you} can do for your \mn{country}. \label{exCountry}}%
\vspace{-0.6cm}
\nnumsentence{\mn{All} for \mn{one}, \mn{one} for \mn{all}. \label{exAll}}%
%

\noindent
That is why we test syntactic features (Papers~II and III) and obtain an overall improvement of 14\% absolute average precision over shallow features.
%This last part has been made thanks to a larger corpus than before.
 However, the difference does not prove to be statistically significant (p>0.05).\footnote{To measure statistical significance, we apply the bootstrapping method of  \cite{Berg-Kirkpatrick2012}.}
\subsection{Epanaphora and Epiphora}
As we said in \Cref{bottomup}, we start from our experience of chiasmus and apply it to other figures.
Thus, the features we test for epanaphora and epiphora are similar to the basic shallow features used for chiasmus. These features are related to number of words, n-gram similarities, and n-gram differences. In the end, we observe an overall improvement of 21\% for epiphora and 38\% for epanaphora in average precision compared to basic features inspired by \cite{Strommer2011}. This time, the corpus and the differences prove to be large enough, and we obtain definitely significant results (p<0.05).

One overall trend we would like to underline at this stage is that none of the experiments in Papers~I--IV need very deep semantic features to build a system with reasonable performance. Despite the fact that some of the examples in our corpus seem to appeal to some sophisticated cognitive process (such as understanding a pun, a parallelism, a paradox, etc.) there was no need to model these deep semantic features to detect the figures. For instance, to detect ``it is not the beginning of the end, but the end of the beginning'' there is no need for a wordnet that would model the notion of opposition between ``beginning'' and ``end''. While this finding is hardly surprising for a well-trained computational linguist, it probably sounds counter-intuitive from a literary point of view. Finally, it is an essential finding from a multilingual perspective: we might be able to develop detectors for under-resourced languages.


\section{Machine Learning with Partially Annotated Corpora}
The bottleneck issue with rhetorical figure detection is the lack of annotated data. To this problem, we must add the fact that the extraction of candidates is noisy and leads to an imbalanced corpus where the true instances are rare, or even extremely rare in the case of chiasmus. Thus we decided to annotate the corpora only partially and in a selective way. This results in only 21\% of the epanaphora candidates being annotated in the training corpus, 15\% in the epiphora training corpus, and not even 1\% in the chiasmus training corpus. The remaining parts of the corpus were labelled as False by default without being seen by any annotator. Despite this, we obtained good results. In particular, with chiasmus, we show that the machine has comparable results to the human in the attribution of weights and in the average precision score (71\% average precision for the computer against 68\% for the human). 

%The partial annotation alloweded us to improve the average precision of the epanaphora and epiphora detection compared to our baseline as we can observe in \Cref{resAna,resEpi}
%\begin{table*}[t!]
%\centering
%
%
%\begin{tabular}{|c|l|l|l|l|r|}
%\hline
%Experiment & Recall  & Precision & F-Score & Av. Prec. \\
%\hline
%Baseline & 30.19 & 29.09 & 29.63 & 19.97 \\
%\hline
%Baseline -Length + DoS & 45.28 & 53.33 & 48.97 & 57.92  \\
%
%\hline
%$\Delta$ & +15 & +14 & +24 & +38  \\
%\hline
%\end{tabular}
%\caption{Results for the epanaphora experiments. Inter annotator agreement Cohen's kappa=85\% \label{resAna}}
%%53 pos instances
%\end{table*}
%
%\begin{table*}[t!]
%\centering
%
%
%\begin{tabular}{|c|l|l|l|l|r|}
%
%\hline
%Experiment & Recall  & Precision & F-Score & Av. Prec. \\
%\hline
%Baseline & 25.71 & 42.86 & 32.14 & 26.78 \\
%\hline
%Full Features & 45.71 & 64.00 & 53.33 & 47.90  \\
%\hline
%
%$\Delta$ & +20 & +21 & +21 & +21  \\
%\hline
%\end{tabular}
%\caption{Results for the epiphora experiments. Inter annotator agreement Cohen's kappa=88\% \label{resEpi}}%35 pos instances
%\end{table*}
%
%The improvement of 38\% and 21\% average precision for epanaphora and epiphora shows that a relatively small but well selected annotation set is enough for improving a system.

\section{Experiments with Additional Genres}

An important question thoughout our study has been to find out if our system can perform on a different type of corpus than the one it has been trained on. A first experiment was performed to detect chiasmus in an anthology of Sherlock Holmes stories. In this case, we discovered that the performance of our system is better than the baseline (+17\% average precision) and the overall result is quite good (70\% average precision with all features of the hand-tuned model). This score has been obtained without any particular tuning targeted at this genre. That could mean either of two things. First, it could mean that our initial choice of corpus, Europarl, was generic enough to allow our system to adapt to a different genre, in this case, novels. Secondly, it could mean that the characteristics of chiasmus, epanaphora and epiphora are not sensitive to the genre in which they appear.

The last test we performed was a real case study between three genres. We crawled web resources to extract titles of works of fiction, titles of scientific articles, and quotations. We ran our detectors for chiasmus, epanaphora and epiphora on the three corpora obtained. Despite the fact that the genres are different from Europarl, the average precision scores remained high (no result below 65\% on any figure or any corpus). Finally, the main finding is that the use of figures differs between genres. First, the quotation genre contained the largest number of figures in general. This was expected, because quotations are by definition appealing and well-written parts of texts, and therefore are likely to contain special stylistic features like figures of speech. What was less expected was the apparent predominance of chiasmus in scientific titles compared to fiction titles. Additionally, we observe the reverse phenomenon for epanaphora and epiphora in fiction titles. Such observations are definitely an interesting finding for literature analysis and our knowledge of genres in general.

Last but not least, our tools prove able to facilitate the collection of enough examples to allow substantial studies of the figures themselves. Indeed, in a tentative study on chiasmus, \citet[p. 22]{rabatel} explains:
\begin{quotation}
Nombre de nos exemples ont été rassemblés lors de la campagne des présidentielles. Le faible rendement de la collecte nous a contraint à intégrer d’autres exemples et à limiter l’analyse pragmatique contextuelle des antimétaboles inscrites dans un genre [...] spécifiques.\footnote{Many of our examples have been collected during the campaign for the presidency. The low yield of the collection has forced us to integrate other examples and to limit the pragmatic contextual analysis of antimetabole falling within a specific [...] genre.}
\end{quotation}
\noindent
If possible \cite{rabatel} initially would have limited his study of the figure to one genre. However, he could not, probably because he had no tool that could speed up the collection process. In total, he quotes 22 different chiasmi extracted from literature, political discourse and newspapers. This is close to the number we have from scientific titles (21 within the top 100 candidates output by the machine). Thus, a linguistic study on chiasmus targeting one genre was not possible before, but has now been made possible by our system. Thanks to our test on other corpora in Papers~II and IV, our systems prove able to be a real help for the literary analyst. It assists with doing the most repetitive part of the work: collecting examples. That leaves more time for analysis. For more concrete insights on what this collection looks like, the reader can find all our input and output files of the genre study files in Paper~IV\footnote{Waterstones (list of titles of fiction books: \url{https://www.waterstones.com}), DBLP (list of titles of scientific articles \citep{Ley2002}) and quotations: \url{https://github.com/alvations/Quotables}} at this address:  \url{http://stp.lingfil.uu.se/~marie/corpus/}.

In this chapter, we have reported our main results and their impact. The next chapter concludes and summarises our contributions and points to future directions of study.